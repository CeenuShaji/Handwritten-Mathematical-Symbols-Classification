{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc7e294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.resnet50.ResNet50(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16009804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Invalid split train[:10%]. Available splits are: ['test', 'train']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m class_names \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;66;03m# [\"dandelion\", \"daisy\", ...] \u001b[39;00m\n\u001b[0;32m      4\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;66;03m# 5\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m test_set, valid_set, train_set \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[:10\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[10\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m:25\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain[25\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m:]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m avg \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling2D()(model\u001b[38;5;241m.\u001b[39moutput) \n\u001b[0;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(n_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(avg) \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py:52\u001b[0m, in \u001b[0;36mdisallow_positional_args.<locals>.disallow_positional_args_dec\u001b[1;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m _check_no_positional(fn, args, ismethod, allowed\u001b[38;5;241m=\u001b[39mallowed)\n\u001b[0;32m     51\u001b[0m _check_required(fn, kwargs)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py:312\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    309\u001b[0m as_dataset_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, in_memory)\n\u001b[0;32m    310\u001b[0m as_dataset_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshuffle_files\u001b[39m\u001b[38;5;124m\"\u001b[39m, shuffle_files)\n\u001b[1;32m--> 312\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mas_dataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_info:\n\u001b[0;32m    314\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ds, dbuilder\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py:52\u001b[0m, in \u001b[0;36mdisallow_positional_args.<locals>.disallow_positional_args_dec\u001b[1;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m _check_no_positional(fn, args, ismethod, allowed\u001b[38;5;241m=\u001b[39mallowed)\n\u001b[0;32m     51\u001b[0m _check_required(fn, kwargs)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:420\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[1;34m(self, split, batch_size, shuffle_files, decoders, as_supervised, in_memory)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m    412\u001b[0m build_single_dataset \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_single_dataset,\n\u001b[0;32m    414\u001b[0m     shuffle_files\u001b[38;5;241m=\u001b[39mshuffle_files,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m     in_memory\u001b[38;5;241m=\u001b[39min_memory,\n\u001b[0;32m    419\u001b[0m )\n\u001b[1;32m--> 420\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_single_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:136\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[0;32m    134\u001b[0m   types\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mtuple\u001b[39m(types)):\n\u001b[1;32m--> 136\u001b[0m   mapped \u001b[38;5;241m=\u001b[39m [map_nested(function, v, dict_only, map_tuple)\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct]\n\u001b[0;32m    138\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:136\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    134\u001b[0m   types\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mtuple\u001b[39m(types)):\n\u001b[1;32m--> 136\u001b[0m   mapped \u001b[38;5;241m=\u001b[39m [\u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct]\n\u001b[0;32m    138\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapped\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:143\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[0;32m    141\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(mapped)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Singleton\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:489\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[1;34m(self, split, shuffle_files, batch_size, decoders, as_supervised, in_memory)\u001b[0m\n\u001b[0;32m    486\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;28mnext\u001b[39m(dataset_utils\u001b[38;5;241m.\u001b[39mas_numpy(dataset)))\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m      \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size:\n\u001b[0;32m    493\u001b[0m   \u001b[38;5;66;03m# Use padded_batch so that features with unknown shape are supported.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mpadded_batch(batch_size, dataset\u001b[38;5;241m.\u001b[39moutput_shapes)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:832\u001b[0m, in \u001b[0;36mFileAdapterBuilder._as_dataset\u001b[1;34m(self, split, decoders, shuffle_files)\u001b[0m\n\u001b[0;32m    828\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfrecords_reader\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    829\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, split, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues(), shuffle_files)\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    831\u001b[0m   \u001b[38;5;66;03m# Resolve all the named split tree by real ones\u001b[39;00m\n\u001b[1;32m--> 832\u001b[0m   read_instruction \u001b[38;5;241m=\u001b[39m \u001b[43msplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_read_instruction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m   \u001b[38;5;66;03m# Extract the list of SlicedSplitInfo objects containing the splits\u001b[39;00m\n\u001b[0;32m    834\u001b[0m   \u001b[38;5;66;03m# to use and their associated slice\u001b[39;00m\n\u001b[0;32m    835\u001b[0m   list_sliced_split_info \u001b[38;5;241m=\u001b[39m read_instruction\u001b[38;5;241m.\u001b[39mget_list_sliced_split_info()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:359\u001b[0m, in \u001b[0;36mNamedSplit.get_read_instruction\u001b[1;34m(self, split_dict)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_read_instruction\u001b[39m(\u001b[38;5;28mself\u001b[39m, split_dict):\n\u001b[1;32m--> 359\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SplitReadInstruction(\u001b[43msplit_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_datasets\\core\\splits.py:533\u001b[0m, in \u001b[0;36mSplitDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m    532\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(key) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid split \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Available splits are: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    534\u001b[0m         key, \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()))))\n\u001b[0;32m    535\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SplitDict, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mstr\u001b[39m(key))\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Invalid split train[:10%]. Available splits are: ['test', 'train']\""
     ]
    }
   ],
   "source": [
    "ds, info = tfds.load(\"mnist\", with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples # 3670 \n",
    "class_names = info.features[\"label\"].names # [\"dandelion\", \"daisy\", ...] \n",
    "n_classes = info.features[\"label\"].num_classes # 5\n",
    "\n",
    "test_set, valid_set, train_set = tfds.load(\"mnist\", split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"])\n",
    " \n",
    "avg = tf.keras.layers.GlobalAveragePooling2D()(model.output) \n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg) \n",
    "model = tf.keras.Model(inputs=model.input, outputs=output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01) \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]) \n",
    "history = model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5322562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <tfds.core.SplitInfo num_examples=10000>,\n",
       " 'train': <tfds.core.SplitInfo num_examples=60000>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328743c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
